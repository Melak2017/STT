{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoreload notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> #### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "#notebook reproducible \n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Directory listing\n",
    "from glob import glob\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pylab as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Audio Processing\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "from pydub import AudioSegment\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile #for audio processing\n",
    "#for pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Color Palettes\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "sns.set_theme(style=\"white\", palette=None)\n",
    "color_pal = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "color_cycle = cycle(plt.rcParams['axes.prop_cycle'].by_key()['color'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> #### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('../scripts')))\n",
    "from data_preprocessing import AudioUtil\n",
    "from data_loader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(os.path.abspath(os.path.join('data')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PATH = '../data/train/wav/'\n",
    "prep = AudioUtil()\n",
    "load = DataLoader(data_dir=PATH, sample_rate=16000, max_duration=12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.insert(0,'../scripts/')\n",
    "from data_loader import DataLoader\n",
    "from resize_audio import resize_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(\"../data/train/wav\")\n",
    "dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = '../data/train/wav/'\n",
    "load = DataLoader(data_dir=PATH, sample_rate=16000, max_duration=12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amharic_audio_files = glob(\"../data/AMHARIC/data/train/wav/*.wav\")\n",
    "amh_audios = load.get_wav_files()[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amh_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(amh_audios[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, sr = librosa.load(amh_audios[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "pd.Series(y).plot(figsize=(16, 4), lw=1, title=\"Raw Data Sample\", color=color_pal[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_trimmed, _ = librosa.effects.trim(y, top_db=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")\n",
    "pd.Series(y_trimmed).plot(figsize=(12, 4), lw=1, title=\"Raw Data Sample\", color=color_pal[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files_all = load.get_wav_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_files_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load transcription and save as json\n",
    "audio_path, text, duration = load.load_transcription(file_path='../data/train/text', dest_path='../data/trainsr_corpus.json', save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_meta_data(path, dest_path):\n",
    "        \"\"\"Generate meta data csv\"\"\"\n",
    "\n",
    "        try:\n",
    "            #self.logger.info('Generating meta data csv')\n",
    "            audio_path, text, duration = load.load_transcription(\n",
    "                path, dest_path)\n",
    "            data = pd.DataFrame(\n",
    "                {'key': audio_path, 'text': text, 'duration': duration})\n",
    "            data.to_csv(dest_path, index=False)\n",
    "            print(\"Meta data creatwd successfully\")\n",
    "            #self.logger.info('Successfully generated meta data csv')\n",
    "        except Exception as e:\n",
    "            #self.logger.error('Failed to generate meta data csv')\n",
    "            # self.logger.error(e)\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = \"../data/train/text\"\n",
    "generate_meta_data(load_path, '../data/trainsr_corpus.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Convert to Channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_stereo(audio_path, dest_path, new_channel) -> None:\n",
    "        \"\"\"Convert the audio to stereo.\"\"\"\n",
    "        try:\n",
    "            for file in audio_path:\n",
    "                # return file\n",
    "                sound = AudioSegment.from_file(file, format=\"wav\")\n",
    "                if sound.channels == new_channel:\n",
    "                    continue\n",
    "                else:\n",
    "                    sound = sound.set_channels(new_channel)\n",
    "\n",
    "                file_name = dest_path + file.split('/')[-1]\n",
    "                sound.export(file_name, format=\"wav\")\n",
    "            # self.logger.info(\"Successfully converted audio to stereo\")\n",
    "        except Exception as e:\n",
    "            # self.logger.error('Failed to convert to stereo')\n",
    "            # self.logger.error(e)\n",
    "            sys.exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep.convert_to_stereo(audio_path=audio_files_all, dest_path='../data/train/', new_channel=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Create meta data to pass the path of the audios to resample it to 44.1 KHZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_data=pd.read_csv(\"../data/train/trsTrain.txt\",sep=\"\\t\",header=None)\n",
    "pipe = Pipeline(steps = [(\"metadata\", FunctionTransformer(prep.create_meta_data, kw_args={\"column1\":'Transcript', \"column2\": 'audio'}))])\n",
    "meta_pipe = pipe.fit_transform(meta_data)\n",
    "meta_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Resampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resampling any loaded audio files to 44.1KHZ \n",
    "prep.resample(meta_data, 'audio')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Resize Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load 100 audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audios, max_duration = dl.load_audios(False,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the durations of the hundred audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = '../data/train/wav/'\n",
    "durations = []\n",
    "labels = os.listdir(train_audio_path)\n",
    "for i,label in enumerate(labels):\n",
    "    if i > 100:\n",
    "        break\n",
    "    samples, sample_rate = librosa.load(train_audio_path+label, sr=44100, mono=False)\n",
    "    durations.append(len(samples)/sample_rate)\n",
    "durations = np.array(durations)\n",
    "plt.hist(durations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_audio(audios,max_duration):\n",
    "    resized_audios = {}\n",
    "    for label in audios:\n",
    "        resized_audios[label] = librosa.util.fix_length(audios[label],size=int(max_duration*44100))\n",
    "    return resized_audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resized_audios = resize_audio(audios,max_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(list(resized_audios.values())[2], rate=44100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ipd.Audio(list(audios.values())[2], rate=44100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Augumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demo_audio = list(audio_files.keys())[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_audio(audios : dict, sample_rate : int) -> dict:\n",
    "  \n",
    "#shift the wave by sample_rate/10 factor.\n",
    "#audios - a dictionary mapping the wav file names to the sampled audio array\n",
    "#sample_rate - the sample rate for the audio\n",
    "#audios - a python dictionary mapping the wav file names to the augmented audio samples\n",
    "  for name in audios:\n",
    "    audios[name] = np.roll(audios[name], int(sample_rate/10))\n",
    "  return audios\n",
    "\n",
    "augment_audio(audio_files, sample_rate)[demo_audio].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_spec(data:np.array,sr:int) -> None:\n",
    "#   '''\n",
    "#   Function for plotting spectrogram along with amplitude wave graph\n",
    "#   '''\n",
    "  \n",
    "#   fig, ax = plt.subplots(1,2,figsize=(15,5))\n",
    "#   ax[0].title.set_text(f'Shfiting the wave by Times {sr/10}')\n",
    "#   ax[0].specgram(data,Fs=2)\n",
    "#   ax[1].set_ylabel('Amplitude')\n",
    "#   ax[1].plot(np.linspace(0,1,len(data)), data)\n",
    "\n",
    "wav_roll = np.roll(samples,int(sample_rate/10))\n",
    "plot_spec(data=wav_roll,sr=sample_rate)\n",
    "ipd.Audio(wav_roll,rate=sample_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every audio signal consists of many features. However, we must extract the characteristics that are relevant to the problem we are trying to solve. The process of extracting features to use them for analysis is called feature extraction. This notebook loads an audio using Librosa library and defines a function to generate spectograms and MFCCs. Spectograms and MFCCs are the features we are intrested in in the audios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Utility Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define utility functions to help us in feature extraction\n",
    "The utility functions defined below are:\n",
    "  \n",
    "\n",
    "1.   **spectogram()** - Computes the spectogram for an audio signal\n",
    "2.   **plot_spectogram()** - This function plots a normalized spectogram of an audio signal\n",
    "3.   **mfcc()** - This function computes the Mel frequency cepstral coefficients (MFCCs) of an audio signal\n",
    "4.   **plot_mfcc()** - This function plots the mfccs of an audio signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_audio_path = '../data/train/wav/'\n",
    "audio, rate = librosa.load(train_audio_path+'tr_10_tr01010.wav')\n",
    "print(\"audio: \", audio)\n",
    "print(\"Sampling rate: \", rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,freqs = prep.spectrogram(audio)\n",
    "freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> spectogram extraction for our selected audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spectrogram is a visual way of representing the signal strength, or “loudness”, of a signal over time at various frequencies present in a particular waveform. Not only can one see whether there is more or less energy at, for example, 2 Hz vs 10 Hz, but one can also see how energy levels vary over time.\n",
    "\n",
    "A spectrogram is usually depicted as a heat map, i.e., as an image with the intensity shown by varying the color or brightness.\n",
    "\n",
    "We can display a spectrogram using. librosa.display.specshow.\n",
    "We compute the spectogram of the audio signal we loaded and plot the spectogram feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,freqs= prep.spectrogram(audio)  # compute the spectrogram of the audio\n",
    "prep.plot_spectrogram_feature(x)  # plot the spectogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> MFCC extraction for our selected audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE compute of the loaded signal and plot the mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from python_speech_features import mfcc\n",
    "mfccs = mfcc(audio, rate) # compute the mfcc\n",
    "prep.plot_mfcc(mfccs, rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Other spectral Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Spectral Centroid**\n",
    "\n",
    "The spectral centroid indicates at which frequency the energy of a spectrum is centered upon or in other words It indicates where the ” center of mass” for a sound is located. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectral_centroids(audio, sampling_rate):\n",
    "  \"\"\"\n",
    "  computes the spectral centroid for each frame in an audio signal and the time variable for visualization\n",
    "  \"\"\"\n",
    "  spectral_centroids = librosa.feature.spectral_centroid(audio, sr=sampling_rate)\n",
    "  frames = range(len(spectral_centroids[0]))\n",
    "  t = librosa.frames_to_time(frames)\n",
    "  return spectral_centroids, t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the spectral centroid for our input signal we loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalising the spectral centroid for visualisation\n",
    "def normalize(x, axis=0):\n",
    "    return sklearn.preprocessing.minmax_scale(x, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_centroid, t = spectral_centroids(audio, rate)\n",
    "spectral_centroid= spectral_centroid[0]\n",
    "\n",
    "#Plotting the Spectral Centroid along the waveform\n",
    "librosa.display.waveplot(audio, sr=rate, alpha=0.4)\n",
    "#plt.figure(figsize=(12, 4))\n",
    "plt.plot(t, normalize(spectral_centroid), color='b')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Spectral Rolloff**\n",
    "\n",
    "It is a measure of the shape of the signal. It represents the frequency at which high frequencies decline to 0. To obtain it, we have to calculate the fraction of bins in the power spectrum where 85% of its power is at lower frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_rolloff = librosa.feature.spectral_rolloff(audio+0.01, sr=rate)[0]\n",
    "#plt.figure(figsize=(12, 4))\n",
    "librosa.display.waveplot(audio, sr=rate, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_rolloff), color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Spectral Bandwidth**\n",
    "\n",
    "The spectral bandwidth is defined as the width of the band of light at one-half the peak maximum (or full width at half maximum [FWHM]) and is represented by the two vertical red lines and λSB on the wavelength axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectral_bandwidth_2 = librosa.feature.spectral_bandwidth(audio+0.01, sr=rate)[0]\n",
    "spectral_bandwidth_3 = librosa.feature.spectral_bandwidth(audio+0.01, sr=rate, p=3)[0]\n",
    "spectral_bandwidth_4 = librosa.feature.spectral_bandwidth(audio+0.01, sr=rate, p=4)[0]\n",
    "#plt.figure(figsize=(15, 9))\n",
    "librosa.display.waveplot(audio, sr=rate, alpha=0.4)\n",
    "plt.plot(t, normalize(spectral_bandwidth_2), color='r')\n",
    "plt.plot(t, normalize(spectral_bandwidth_3), color='g')\n",
    "plt.plot(t, normalize(spectral_bandwidth_4), color='y')\n",
    "plt.legend(('p = 2', 'p = 3', 'p = 4'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Chroma feature**\n",
    "\n",
    "A chroma feature or vector is typically a 12-element feature vector indicating how much energy of each pitch class, {C, C#, D, D#, E, …, B}, is present in the signal. In short, It provides a robust way to describe a similarity measure between music pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromagram = librosa.feature.chroma_stft(audio, sr=rate, hop_length=512)\n",
    "#plt.figure(figsize=(15, 5))\n",
    "librosa.display.specshow(chromagram, x_axis='time', y_axis='chroma', hop_length=512, cmap='coolwarm')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
